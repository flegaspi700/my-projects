{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is:  ['fml-gcp-project-01']\n"
     ]
    }
   ],
   "source": [
    "PROJECT=!gcloud config get-value core/project \r\n",
    "#gcloud config list project --format \"value(core.project)\"\r\n",
    "print(\"Your current GCP Project Name is: \", PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil mb gs://pedok-ml-gcp-vision-api\r\n",
    "#!gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable vision.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.iam.service-accounts.create) Resource in projects [fml-gcp-project-01] is the subject of a conflict: Service account my-vision-sa already exists within project projects/fml-gcp-project-01.\n",
      "- '@type': type.googleapis.com/google.rpc.ResourceInfo\n",
      "  resourceName: projects/fml-gcp-project-01/serviceAccounts/my-vision-sa@fml-gcp-project-01.iam.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts create my-vision-sa --display-name \"my vision service account\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "created key [9f3ad220cd059052f2b2537b51b2aad2423bb1d0] of type [json] as [key.json] for [my-vision-sa@fml-gcp-project-01.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts keys create key.json --iam-account my-vision-sa@fml-gcp-project-01.iam.gserviceaccount.com\r\n",
    "#!gcloud iam service-accounts keys create ~/key.json --iam-account my-vision-sa@{PROJECT_ID}.iam.gserviceaccount.com\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set GOOGLE_APPLICATION_CREDENTIALS=\"D:\\Portfolio\\my-projects\\GCP\\key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (21.1.2)\n",
      "Requirement already satisfied: google-cloud-vision in c:\\users\\cttin\\appdata\\roaming\\python\\python39\\site-packages (2.3.1)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-vision) (1.26.3)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in c:\\users\\cttin\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-vision) (1.18.1)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\cttin\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.15.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.25.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.13.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.53.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2020.4)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.28.1)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (20.7)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (49.2.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.34.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.2.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\cttin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U pip google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2021-05-23T20:04:09.910Z'\n",
      "lifecycleState: ACTIVE\n",
      "name: fml-gcp-project-01\n",
      "projectId: fml-gcp-project-01\n",
      "projectNumber: '318144289060'\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects describe fml-gcp-project-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels (and confidence score):\n",
      "==============================\n",
      "Wheel (97.90%)\n",
      "Tire (97.85%)\n",
      "Bicycle (94.53%)\n",
      "Photograph (94.24%)\n",
      "Motor vehicle (91.37%)\n",
      "Infrastructure (89.83%)\n",
      "Bicycle wheel (85.65%)\n",
      "Mode of transport (85.51%)\n",
      "Asphalt (82.82%)\n",
      "Umbrella (81.78%)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\r\n",
    "\r\n",
    "image_uri = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\r\n",
    "\r\n",
    "client = vision.ImageAnnotatorClient()\r\n",
    "image = vision.Image()\r\n",
    "image.source.image_uri = image_uri\r\n",
    "\r\n",
    "response = client.label_detection(image=image)\r\n",
    "\r\n",
    "print('Labels (and confidence score):')\r\n",
    "print('=' * 30)\r\n",
    "for label in response.label_annotations:\r\n",
    "    print(label.description, '(%.2f%%)' % (label.score*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Perform Text Detection\r\n",
    "Text detection performs Optical Character Recognition (OCR). It detects and extracts text within an image with support for a broad range of languages. It also features automatic language identification.\r\n",
    "\r\n",
    "In this example, you will perform text detection on an image of an Otter Crossing. Copy the following snippet into your IPython session (or save locally as text_dectect.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "CAUTION\n",
      "Otters crossing\n",
      "for next 6 miles\n",
      "\n",
      "bounds: (61,243),(251,243),(251,340),(61,340)\n",
      "==============================\n",
      "CAUTION\n",
      "bounds: (75,245),(235,243),(235,269),(75,271)\n",
      "==============================\n",
      "Otters\n",
      "bounds: (65,296),(140,297),(140,315),(65,314)\n",
      "==============================\n",
      "crossing\n",
      "bounds: (151,295),(247,297),(247,318),(151,316)\n",
      "==============================\n",
      "for\n",
      "bounds: (61,322),(94,322),(94,340),(61,340)\n",
      "==============================\n",
      "next\n",
      "bounds: (106,322),(156,322),(156,340),(106,340)\n",
      "==============================\n",
      "6\n",
      "bounds: (167,321),(180,321),(180,339),(167,339)\n",
      "==============================\n",
      "miles\n",
      "bounds: (191,321),(251,321),(251,339),(191,339)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "image_uri = 'gs://cloud-vision-codelab/otter_crossing.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.text_detection(image=image)\n",
    "\n",
    "for text in response.text_annotations:\n",
    "    print('=' * 30)\n",
    "    print(text.description)\n",
    "    vertices = ['(%s,%s)' % (v.x, v.y) for v in text.bounding_poly.vertices]\n",
    "    print('bounds:', \",\".join(vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Perform Landmark Detection\n",
    "Landmark detection detects popular natural and man-made structures within an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "mid: \"/g/120xtw6z\"\n",
      "description: \"Trocad\\303\\251ro Gardens\"\n",
      "score: 0.9215528\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 339\n",
      "    y: 54\n",
      "  }\n",
      "  vertices {\n",
      "    x: 520\n",
      "    y: 54\n",
      "  }\n",
      "  vertices {\n",
      "    x: 520\n",
      "    y: 353\n",
      "  }\n",
      "  vertices {\n",
      "    x: 339\n",
      "    y: 353\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.861596299999995\n",
      "    longitude: 2.2892823\n",
      "  }\n",
      "}\n",
      "\n",
      "==============================\n",
      "mid: \"/m/02j81\"\n",
      "description: \"Eiffel Tower\"\n",
      "score: 0.65712523\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 458\n",
      "    y: 144\n",
      "  }\n",
      "  vertices {\n",
      "    x: 495\n",
      "    y: 144\n",
      "  }\n",
      "  vertices {\n",
      "    x: 495\n",
      "    y: 255\n",
      "  }\n",
      "  vertices {\n",
      "    x: 458\n",
      "    y: 255\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.858461\n",
      "    longitude: 2.294351\n",
      "  }\n",
      "}\n",
      "\n",
      "==============================\n",
      "mid: \"/m/02j81\"\n",
      "description: \"Eiffel Tower\"\n",
      "score: 0.65595245\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 448\n",
      "    y: 72\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 72\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 281\n",
      "  }\n",
      "  vertices {\n",
      "    x: 448\n",
      "    y: 281\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.875072\n",
      "    longitude: 2.312622\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "image_uri = 'gs://cloud-vision-codelab/eiffel_tower.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.landmark_detection(image=image)\n",
    "\n",
    "for landmark in response.landmark_annotations:\n",
    "    print('=' * 30)\n",
    "    print(landmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Perform Emotional Face Detection\r\n",
    "Facial features detection detects multiple faces within an image along with the associated key facial attributes such as emotional state or wearing headwear.\r\n",
    "\r\n",
    "In this example, you will detect the likelihood of emotional state from four different emotional likelihoods including: joy, anger, sorrow, and surprise.\r\n",
    "\r\n",
    "To perform emotional face detection, copy the following Python code into your IPython session (or save locally as face_dectect.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "File: face_surprise.jpg\n",
      "Face surprised: LIKELY\n",
      "Face bounds: (93,425),(520,425),(520,922),(93,922)\n",
      "==============================\n",
      "File: face_no_surprise.png\n",
      "Face surprised: VERY_UNLIKELY\n",
      "Face bounds: (120,0),(334,0),(334,198),(120,198)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "uri_base = 'gs://cloud-vision-codelab'\n",
    "pics = ('face_surprise.jpg', 'face_no_surprise.png')\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "\n",
    "for pic in pics:\n",
    "    image.source.image_uri = '%s/%s' % (uri_base, pic)\n",
    "    response = client.face_detection(image=image)\n",
    "\n",
    "    print('=' * 30)\n",
    "    print('File:', pic)\n",
    "    for face in response.face_annotations:\n",
    "        likelihood = vision.Likelihood(face.surprise_likelihood)\n",
    "        vertices = ['(%s,%s)' % (v.x, v.y) for v in face.bounding_poly.vertices]\n",
    "        print('Face surprised:', likelihood.name)\n",
    "        print('Face bounds:', \",\".join(vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/googlecodelabs/cloud-vision-python\r\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\r\n",
    "12. Additional Resources\r\n",
    "In addition to the source code for the four examples you completed in this codelab, below are additional reading material as well as recommended exercises to augment your knowledge and use of the Vision API with Python.\r\n",
    "\r\n",
    "Learn More\r\n",
    "Cloud Vision API documentation: cloud.google.com/vision/docs\r\n",
    "Cloud Vision API home page & live demo: cloud.google.com/vision\r\n",
    "Vision API label detection/annotation: cloud.google.com/vision/docs/labels\r\n",
    "Vision API facial feature recognition: cloud.google.com/vision/docs/detecting-faces\r\n",
    "Vision API landmark detection: cloud.google.com/vision/docs/detecting-landmarks\r\n",
    "Vision API optical character recognition (OCR): cloud.google.com/vision/docs/ocr\r\n",
    "Vision API \"Safe Search\": cloud.google.com/vision/docs/detecting-safe-search\r\n",
    "Vision API product/corporate logo detection: cloud.google.com/vision/docs/detecting-logos\r\n",
    "Python on Google Cloud Platform: cloud.google.com/python\r\n",
    "Google Cloud Python client: googlecloudplatform.github.io/google-cloud-python\r\n",
    "Codelab open source repo: github.com/googlecodelabs/cloud-vision-\r\n",
    "\r\n",
    "Additional Study\r\n",
    "Now that you have some experience with the Vision API under your belt, below are some recommended exercises to further develop your skills:\r\n",
    "\r\n",
    "You've built separate scripts demoing individual features of the Vision API. \r\n",
    "Combine at least 2 of them into another script. For example, add OCR/text recognition to the first script that performs label detection (label_detect.py). \r\n",
    "You may be surprised to find there is text on one of the hats in that image!\r\n",
    "\r\n",
    "Instead of our random images available on Google Cloud Storage, write a script that uses one or more of your images on your local filesystem. Another similar exercise is to find images online (accessible via http://).\r\n",
    "Same as #2, but with local images on your filesystem. Note that #2 may be an easier first step before doing this one with local files.\r\n",
    "\r\n",
    "Try non-photographs to see how the API works with those.\r\n",
    "\r\n",
    "Migrate some of the script functionality into a microservice hosted on Google Cloud Functions, or in a web app or mobile backend running on Google App Engine.\r\n",
    "\r\n",
    "If you're ready to tackle that last suggestion but can't think of any ideas, here are a pair to get your gears going:\r\n",
    "\r\n",
    "Analyze multiple images in a Cloud Storage bucket, a Google Drive folder (use the Drive API), or a directory on your local computer. Call the Vision API on each image, writing out data about each into a Google Sheet (use the Sheets API) or Excel spreadsheet. (NOTE: you may have to do some extra auth work as G Suite assets like Drive folders and Sheets spreadsheets generally belong to users, not service accounts.)\r\n",
    "\r\n",
    "Some people Tweet images (phone screenshots) of other tweets where the text of the original can't be cut-n-pasted or otherwise analyzed. Use the Twitter API to retrieve the referring tweet, extract and pass the tweeted image to the Vision API to OCR the text out of those images, then call the Cloud Natural Language API to perform sentiment analysis (to determine whether it's positive or negative) and entity extraction (search for entities/proper nouns) on them. (This is optional for the text in the referring tweet.)\r\n",
    "License\r\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "keywords must be strings",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f4221bc3c6f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#image.source.image_uri = image_uri\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.label_detection(image=image)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#print('Labels (and confidence score):')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\vision_helpers\\__init__.py\u001b[0m in \u001b[0;36mannotate_image\u001b[1;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_all_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         r = self.batch_annotate_images(\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[0mrequests\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\vision_v1\\services\\image_annotator\\client.py\u001b[0m in \u001b[0;36mbatch_annotate_images\u001b[1;34m(self, request, requests, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrequests\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mrequests\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_annotator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAnnotateImageRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;31m# If we have keyword arguments corresponding to fields on the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\proto\\message.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m                 )\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m             \u001b[0mpb_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarshal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpb_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpb_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpb_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\proto\\marshal\\marshal.py\u001b[0m in \u001b[0;36mto_proto\u001b[1;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;31m# Convert lists and tuples recursively.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;31m# Convert dictionaries recursively when the proto type is a map.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\proto\\marshal\\marshal.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;31m# Convert lists and tuples recursively.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;31m# Convert dictionaries recursively when the proto type is a map.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\proto\\marshal\\marshal.py\u001b[0m in \u001b[0;36mto_proto\u001b[1;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;31m# Convert ordinary values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mrule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mpb_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;31m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\proto\\marshal\\rules\\message.py\u001b[0m in \u001b[0;36mto_proto\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_descriptor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: keywords must be strings"
     ]
    }
   ],
   "source": [
    "#Combine at least 2 of them into another script. For example, \r\n",
    "# add OCR/text recognition to the first script that performs label detection (label_detect.py). \r\n",
    "\r\n",
    "from google.cloud import vision_v1\r\n",
    "\r\n",
    "image_uri = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\r\n",
    "\r\n",
    "client = vision_v1.ImageAnnotatorClient()\r\n",
    "\r\n",
    "request = {\r\n",
    "  'image': {'source': {'image_uri': 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'}},\r\n",
    "  'features': [{type: vision_v1.types.Feature.Type.LABEL_DETECTION}, \r\n",
    "        {type: vision_v1.types.Feature.Type.TEXT_DETECTION}]\r\n",
    "};\r\n",
    "#image = vision.Image()\r\n",
    "#image.source.image_uri = image_uri\r\n",
    "\r\n",
    "response = client.annotate_image(request) #.label_detection(image=image)\r\n",
    "\r\n",
    "#print('Labels (and confidence score):')\r\n",
    "#print('=' * 30)\r\n",
    "#for label in response.label_annotations:\r\n",
    "#    print(label.description, '(%.2f%%)' % (label.score*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\r\n",
    "Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Unrestricted\r\n",
    "\r\n",
    "!pip install --user apache-beam[gcp]==2.16.0 \r\n",
    "!pip install --user tensorflow-transform==0.15.\r\n",
    "\r\n",
    "BUCKET = 'qwiklabs-gcp-01-89767ecc5125'\r\n",
    "PROJECT = 'qwiklabs-gcp-01-89767ecc5125'\r\n",
    "REGION = 'us-central1'\r\n",
    "\r\n",
    "import os\r\n",
    "os.environ['BUCKET'] = BUCKET\r\n",
    "os.environ['PROJECT'] = PROJECT\r\n",
    "os.environ['REGION'] = REGION\r\n",
    "\r\n",
    "gcloud config set project $PROJECT\r\n",
    "gcloud config set compute/region $REGION\r\n",
    "\r\n",
    "%%bash\r\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\r\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\r\n",
    "fi\r\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python391jvsc74a57bd08f0e6e8876776058453f48ad7c7dcdebdb8994a74dd96d3c38b5ba45b7b9008d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}