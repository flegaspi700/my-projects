{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is:  ['fml-gcp-project-01']\n"
     ]
    }
   ],
   "source": [
    "PROJECT=!gcloud config get-value core/project \r\n",
    "#gcloud config list project --format \"value(core.project)\"\r\n",
    "print(\"Your current GCP Project Name is: \", PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating gs://pedok-ml-gcp-vision-api/...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://pedok-ml-gcp-vision-api/\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb gs://pedok-ml-gcp-vision-api\r\n",
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable vision.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud iam service-accounts create my-vision-sa --display-name \"my vision service account\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "created key [9a0bbdfea00c14193e77b1873e25db3263e6e40e] of type [json] as [key.json] for [my-vision-sa@fml-gcp-project-01.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts keys create key.json --iam-account my-vision-sa@fml-gcp-project-01.iam.gserviceaccount.com\r\n",
    "#!gcloud iam service-accounts keys create ~/key.json --iam-account my-vision-sa@{PROJECT_ID}.iam.gserviceaccount.com\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set GOOGLE_APPLICATION_CREDENTIALS=\"D:\\Portfolio\\my-projects\\GCP\\key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U pip google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2021-05-23T20:04:09.910Z'\n",
      "lifecycleState: ACTIVE\n",
      "name: fml-gcp-project-01\n",
      "projectId: fml-gcp-project-01\n",
      "projectNumber: '318144289060'\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects describe fml-gcp-project-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels (and confidence score):\n",
      "==============================\n",
      "Wheel (97.90%)\n",
      "Tire (97.85%)\n",
      "Bicycle (94.53%)\n",
      "Photograph (94.24%)\n",
      "Motor vehicle (91.37%)\n",
      "Infrastructure (89.83%)\n",
      "Bicycle wheel (85.65%)\n",
      "Mode of transport (85.51%)\n",
      "Asphalt (82.82%)\n",
      "Umbrella (81.78%)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\r\n",
    "\r\n",
    "image_uri = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\r\n",
    "\r\n",
    "client = vision.ImageAnnotatorClient()\r\n",
    "image = vision.Image()\r\n",
    "image.source.image_uri = image_uri\r\n",
    "\r\n",
    "response = client.label_detection(image=image)\r\n",
    "\r\n",
    "print('Labels (and confidence score):')\r\n",
    "print('=' * 30)\r\n",
    "for label in response.label_annotations:\r\n",
    "    print(label.description, '(%.2f%%)' % (label.score*100.))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Perform Text Detection\r\n",
    "Text detection performs Optical Character Recognition (OCR). It detects and extracts text within an image with support for a broad range of languages. It also features automatic language identification.\r\n",
    "\r\n",
    "In this example, you will perform text detection on an image of an Otter Crossing. Copy the following snippet into your IPython session (or save locally as text_dectect.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "CAUTION\n",
      "Otters crossing\n",
      "for next 6 miles\n",
      "\n",
      "bounds: (61,243),(251,243),(251,340),(61,340)\n",
      "==============================\n",
      "CAUTION\n",
      "bounds: (75,245),(235,243),(235,269),(75,271)\n",
      "==============================\n",
      "Otters\n",
      "bounds: (65,296),(140,297),(140,315),(65,314)\n",
      "==============================\n",
      "crossing\n",
      "bounds: (151,295),(247,297),(247,318),(151,316)\n",
      "==============================\n",
      "for\n",
      "bounds: (61,322),(94,322),(94,340),(61,340)\n",
      "==============================\n",
      "next\n",
      "bounds: (106,322),(156,322),(156,340),(106,340)\n",
      "==============================\n",
      "6\n",
      "bounds: (167,321),(180,321),(180,339),(167,339)\n",
      "==============================\n",
      "miles\n",
      "bounds: (191,321),(251,321),(251,339),(191,339)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "image_uri = 'gs://cloud-vision-codelab/otter_crossing.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.text_detection(image=image)\n",
    "\n",
    "for text in response.text_annotations:\n",
    "    print('=' * 30)\n",
    "    print(text.description)\n",
    "    vertices = ['(%s,%s)' % (v.x, v.y) for v in text.bounding_poly.vertices]\n",
    "    print('bounds:', \",\".join(vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Perform Landmark Detection\n",
    "Landmark detection detects popular natural and man-made structures within an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "mid: \"/g/120xtw6z\"\n",
      "description: \"Trocad\\303\\251ro Gardens\"\n",
      "score: 0.91856456\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 339\n",
      "    y: 54\n",
      "  }\n",
      "  vertices {\n",
      "    x: 531\n",
      "    y: 54\n",
      "  }\n",
      "  vertices {\n",
      "    x: 531\n",
      "    y: 371\n",
      "  }\n",
      "  vertices {\n",
      "    x: 339\n",
      "    y: 371\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.861596299999995\n",
      "    longitude: 2.2892823\n",
      "  }\n",
      "}\n",
      "\n",
      "==============================\n",
      "mid: \"/m/02j81\"\n",
      "description: \"Eiffel Tower\"\n",
      "score: 0.6049596\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 448\n",
      "    y: 180\n",
      "  }\n",
      "  vertices {\n",
      "    x: 531\n",
      "    y: 180\n",
      "  }\n",
      "  vertices {\n",
      "    x: 531\n",
      "    y: 299\n",
      "  }\n",
      "  vertices {\n",
      "    x: 448\n",
      "    y: 299\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.858461\n",
      "    longitude: 2.294351\n",
      "  }\n",
      "}\n",
      "\n",
      "==============================\n",
      "mid: \"/m/02j81\"\n",
      "description: \"Eiffel Tower\"\n",
      "score: 0.5975549\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 448\n",
      "    y: 76\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 76\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 281\n",
      "  }\n",
      "  vertices {\n",
      "    x: 448\n",
      "    y: 281\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.875072\n",
      "    longitude: 2.312622\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "image_uri = 'gs://cloud-vision-codelab/eiffel_tower.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.landmark_detection(image=image)\n",
    "\n",
    "for landmark in response.landmark_annotations:\n",
    "    print('=' * 30)\n",
    "    print(landmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Perform Emotional Face Detection\r\n",
    "Facial features detection detects multiple faces within an image along with the associated key facial attributes such as emotional state or wearing headwear.\r\n",
    "\r\n",
    "In this example, you will detect the likelihood of emotional state from four different emotional likelihoods including: joy, anger, sorrow, and surprise.\r\n",
    "\r\n",
    "To perform emotional face detection, copy the following Python code into your IPython session (or save locally as face_dectect.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "File: face_surprise.jpg\n",
      "Face surprised: LIKELY\n",
      "Face bounds: (93,425),(520,425),(520,922),(93,922)\n",
      "==============================\n",
      "File: face_no_surprise.png\n",
      "Face surprised: VERY_UNLIKELY\n",
      "Face bounds: (120,0),(334,0),(334,198),(120,198)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "uri_base = 'gs://cloud-vision-codelab'\n",
    "pics = ('face_surprise.jpg', 'face_no_surprise.png')\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "\n",
    "for pic in pics:\n",
    "    image.source.image_uri = '%s/%s' % (uri_base, pic)\n",
    "    response = client.face_detection(image=image)\n",
    "\n",
    "    print('=' * 30)\n",
    "    print('File:', pic)\n",
    "    for face in response.face_annotations:\n",
    "        likelihood = vision.Likelihood(face.surprise_likelihood)\n",
    "        vertices = ['(%s,%s)' % (v.x, v.y) for v in face.bounding_poly.vertices]\n",
    "        print('Face surprised:', likelihood.name)\n",
    "        print('Face bounds:', \",\".join(vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/googlecodelabs/cloud-vision-python\r\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\r\n",
    "12. Additional Resources\r\n",
    "In addition to the source code for the four examples you completed in this codelab, below are additional reading material as well as recommended exercises to augment your knowledge and use of the Vision API with Python.\r\n",
    "\r\n",
    "Learn More\r\n",
    "Cloud Vision API documentation: cloud.google.com/vision/docs\r\n",
    "Cloud Vision API home page & live demo: cloud.google.com/vision\r\n",
    "Vision API label detection/annotation: cloud.google.com/vision/docs/labels\r\n",
    "Vision API facial feature recognition: cloud.google.com/vision/docs/detecting-faces\r\n",
    "Vision API landmark detection: cloud.google.com/vision/docs/detecting-landmarks\r\n",
    "Vision API optical character recognition (OCR): cloud.google.com/vision/docs/ocr\r\n",
    "Vision API \"Safe Search\": cloud.google.com/vision/docs/detecting-safe-search\r\n",
    "Vision API product/corporate logo detection: cloud.google.com/vision/docs/detecting-logos\r\n",
    "Python on Google Cloud Platform: cloud.google.com/python\r\n",
    "Google Cloud Python client: googlecloudplatform.github.io/google-cloud-python\r\n",
    "Codelab open source repo: github.com/googlecodelabs/cloud-vision-\r\n",
    "\r\n",
    "Additional Study\r\n",
    "Now that you have some experience with the Vision API under your belt, below are some recommended exercises to further develop your skills:\r\n",
    "\r\n",
    "You've built separate scripts demoing individual features of the Vision API. \r\n",
    "Combine at least 2 of them into another script. For example, add OCR/text recognition to the first script that performs label detection (label_detect.py). \r\n",
    "You may be surprised to find there is text on one of the hats in that image!\r\n",
    "\r\n",
    "Instead of our random images available on Google Cloud Storage, write a script that uses one or more of your images on your local filesystem. \r\n",
    "Another similar exercise is to find images online (accessible via http://).\r\n",
    "Same as #2, but with local images on your filesystem. Note that #2 may be an easier first step before doing this one with local files.\r\n",
    "\r\n",
    "Try non-photographs to see how the API works with those.\r\n",
    "\r\n",
    "Migrate some of the script functionality into a microservice hosted on Google Cloud Functions, \r\n",
    "or in a web app or mobile backend running on Google App Engine.\r\n",
    "\r\n",
    "If you're ready to tackle that last suggestion but can't think of any ideas, here are a pair to get your gears going:\r\n",
    "\r\n",
    "Analyze multiple images in a Cloud Storage bucket, a Google Drive folder (use the Drive API), \r\n",
    "or a directory on your local computer. Call the Vision API on each image, writing out data about each into a \r\n",
    "Google Sheet (use the Sheets API) or Excel spreadsheet. \r\n",
    "(NOTE: you may have to do some extra auth work as G Suite assets like Drive folders and Sheets spreadsheets \r\n",
    "generally belong to users, not service accounts.)\r\n",
    "\r\n",
    "Some people Tweet images (phone screenshots) of other tweets where the text of the original can't be cut-n-pasted or \r\n",
    "otherwise analyzed. Use the Twitter API to retrieve the referring tweet, extract and pass the tweeted image to the \r\n",
    "Vision API to OCR the text out of those images, then call the Cloud Natural Language API to perform sentiment analysis \r\n",
    "(to determine whether it's positive or negative) and entity extraction (search for entities/proper nouns) on them.\r\n",
    " (This is optional for the text in the referring tweet.)\r\n",
    "License\r\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "Output written to GCS with prefix: gs://pedok-ml-gcp-vision-api/\n"
     ]
    }
   ],
   "source": [
    "#Combine at least 2 of them into another script. For example, \r\n",
    "# add OCR/text recognition to the first script that performs label detection (label_detect.py). \r\n",
    "\r\n",
    "from google.cloud import vision_v1\r\n",
    "\r\n",
    "#image_uri = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\r\n",
    "\r\n",
    "\r\n",
    "input_image_uri=\"gs://cloud-samples-data/vision/using_curl/shanghai.jpeg\"\r\n",
    "output_uri=\"gs://pedok-ml-gcp-vision-api/\"\r\n",
    "\r\n",
    "\"\"\"Perform async batch image annotation.\"\"\"\r\n",
    "client = vision_v1.ImageAnnotatorClient()\r\n",
    "\r\n",
    "source = {\"image_uri\": input_image_uri}\r\n",
    "image = {\"source\": source}\r\n",
    "features = [\r\n",
    "        {\"type_\": vision_v1.Feature.Type.LABEL_DETECTION},\r\n",
    "        {\"type_\": vision_v1.Feature.Type.TEXT_DETECTION},\r\n",
    "    ]\r\n",
    "\r\n",
    "# Each requests element corresponds to a single image.  To annotate more\r\n",
    "# images, create a request element for each image and add it to\r\n",
    "# the array of requests\r\n",
    "requests = [{\"image\": image, \"features\": features}]\r\n",
    "gcs_destination = {\"uri\": output_uri}\r\n",
    "\r\n",
    "# The max number of responses to output in each JSON file\r\n",
    "batch_size = 2\r\n",
    "output_config = {\"gcs_destination\": gcs_destination,\r\n",
    "                     \"batch_size\": batch_size}\r\n",
    "\r\n",
    "operation = client.async_batch_annotate_images(requests=requests, output_config=output_config)\r\n",
    "\r\n",
    "print(\"Waiting for operation to complete...\")\r\n",
    "response = operation.result(90)\r\n",
    "\r\n",
    "# The output is written to GCS with the provided output_uri as prefix\r\n",
    "gcs_output_uri = response.output_config.gcs_destination.uri\r\n",
    "print(\"Output written to GCS with prefix: {}\".format(gcs_output_uri))\r\n",
    "    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_config {\n",
      "  gcs_destination {\n",
      "    uri: \"gs://pedok-ml-gcp-vision-api/\"\n",
      "  }\n",
      "  batch_size: 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(operation.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get() expected a dict or protobuf message, got <class 'list'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-82a1b8f14d71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"features\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\vision_helpers\\__init__.py\u001b[0m in \u001b[0;36mannotate_image\u001b[1;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;31m# If the image is a file handler, set the content.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprotobuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"image\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\protobuf_helpers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(msg_or_dict, key, default)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsg_or_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m    186\u001b[0m             \"get() expected a dict or protobuf message, got {!r}.\".format(\n\u001b[0;32m    187\u001b[0m                 \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_or_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get() expected a dict or protobuf message, got <class 'list'>."
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\r\n",
    "\r\n",
    "client = vision.ImageAnnotatorClient()\r\n",
    "image_path = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\r\n",
    "\r\n",
    "source = {\"image_uri\": image_path}\r\n",
    "image = {\"source\": source}\r\n",
    "features = [\r\n",
    "        {\"type\": vision_v1.Feature.Type.LABEL_DETECTION},\r\n",
    "        {\"type\": vision_v1.Feature.Type.TEXT_DETECTION},\r\n",
    "    ]\r\n",
    "\r\n",
    "query = [{\"image\": image, \"features\": features}]\r\n",
    "\r\n",
    "response = client.annotate_image(query)\r\n",
    "print(response.annotations)\r\n",
    "\r\n",
    "#print('Labels (and confidence score):')\r\n",
    "#print('=' * 30)\r\n",
    "#for label in response.label_annotations:\r\n",
    "#    print(label.description, '(%.2f%%)' % (label.score*100.))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\r\n",
    "Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Unrestricted\r\n",
    "\r\n",
    "!pip install --user apache-beam[gcp]==2.16.0 \r\n",
    "!pip install --user tensorflow-transform==0.15.\r\n",
    "\r\n",
    "BUCKET = 'qwiklabs-gcp-01-89767ecc5125'\r\n",
    "PROJECT = 'qwiklabs-gcp-01-89767ecc5125'\r\n",
    "REGION = 'us-central1'\r\n",
    "\r\n",
    "import os\r\n",
    "os.environ['BUCKET'] = BUCKET\r\n",
    "os.environ['PROJECT'] = PROJECT\r\n",
    "os.environ['REGION'] = REGION\r\n",
    "\r\n",
    "gcloud config set project $PROJECT\r\n",
    "gcloud config set compute/region $REGION\r\n",
    "\r\n",
    "%%bash\r\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\r\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\r\n",
    "fi\r\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python391jvsc74a57bd08f0e6e8876776058453f48ad7c7dcdebdb8994a74dd96d3c38b5ba45b7b9008d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}