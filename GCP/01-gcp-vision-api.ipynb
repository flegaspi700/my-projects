{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!virtualenv my_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!virtualenv -p python3 my_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!source my_env/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!gcloud services enable vision.googleapis.com\r\n",
    "\r\n",
    "!export PROJECT_ID=$(gcloud config get-value core/project)\r\n",
    "\r\n",
    "!gcloud iam service-accounts create my-vision-sa \\\r\n",
    "  --display-name \"my vision service account\"\r\n",
    "\r\n",
    "!gcloud iam service-accounts keys create ~/key.json \\\r\n",
    "  --iam-account my-vision-sa@${PROJECT_ID}.iam.gserviceaccount.com\r\n",
    "\r\n",
    "!export GOOGLE_APPLICATION_CREDENTIALS=~/key.json\r\n",
    "\r\n",
    "#verify if Install the Cloud Vision client library for Python\r\n",
    "!pip3 freeze | grep google-cloud-vision #google-cloud-vision==2.0.0\r\n",
    "\r\n",
    "!pip3 install -U pip google-cloud-vision\r\n",
    "\r\n",
    "#Confirm the client library can be imported without issue like the below, and then you're ready to use the Vision API from real code!\r\n",
    "!python3 -c \"import google.cloud.vision\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Perform Label Detection\n",
    "One of the Vision API's basic features is to identify objects or entities in an image, known as label annotation. Label detection identifies general objects, locations, activities, animal species, products, and more. The Vision API takes an input image and returns the most likely labels which apply to that image. It returns the top-matching labels along with a confidence score of a match to the image.\n",
    "\n",
    "In this example, you will perform label detection on an image of a street scene in Shanghai. To do this, copy the following Python code into your IPython session (or drop it into a local file such as label_detect.py and run it normally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is:  ['fml-gcp-project-01']\n"
     ]
    }
   ],
   "source": [
    "PROJECT=!gcloud config list project --format \"value(core.project)\"\r\n",
    "print(\"Your current GCP Project Name is: \", PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.cloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4cce8bbed37e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimage_uri\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.cloud'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "image_uri = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.label_detection(image=image)\n",
    "\n",
    "print('Labels (and confidence score):')\n",
    "print('=' * 30)\n",
    "for label in response.label_annotations:\n",
    "    print(label.description, '(%.2f%%)' % (label.score*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Perform Text Detection\n",
    "Text detection performs Optical Character Recognition (OCR). It detects and extracts text within an image with support for a broad range of languages. It also features automatic language identification.\n",
    "\n",
    "In this example, you will perform text detection on an image of an Otter Crossing. Copy the following snippet into your IPython session (or save locally as text_dectect.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "image_uri = 'gs://cloud-vision-codelab/otter_crossing.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.text_detection(image=image)\n",
    "\n",
    "for text in response.text_annotations:\n",
    "    print('=' * 30)\n",
    "    print(text.description)\n",
    "    vertices = ['(%s,%s)' % (v.x, v.y) for v in text.bounding_poly.vertices]\n",
    "    print('bounds:', \",\".join(vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Perform Landmark Detection\n",
    "Landmark detection detects popular natural and man-made structures within an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "image_uri = 'gs://cloud-vision-codelab/eiffel_tower.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.landmark_detection(image=image)\n",
    "\n",
    "for landmark in response.landmark_annotations:\n",
    "    print('=' * 30)\n",
    "    print(landmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Perform Emotional Face Detection\n",
    "Facial features detection detects multiple faces within an image along with the associated key facial attributes such as emotional state or wearing headwear.\n",
    "\n",
    "In this example, you will detect the likelihood of emotional state from four different emotional likelihoods including: joy, anger, sorrow, and surprise.\n",
    "\n",
    "To perform emotional face detection, copy the following Python code into your IPython session (or save locally as face_dectect.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "uri_base = 'gs://cloud-vision-codelab'\n",
    "pics = ('face_surprise.jpg', 'face_no_surprise.png')\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "\n",
    "for pic in pics:\n",
    "    image.source.image_uri = '%s/%s' % (uri_base, pic)\n",
    "    response = client.face_detection(image=image)\n",
    "\n",
    "    print('=' * 30)\n",
    "    print('File:', pic)\n",
    "    for face in response.face_annotations:\n",
    "        likelihood = vision.Likelihood(face.surprise_likelihood)\n",
    "        vertices = ['(%s,%s)' % (v.x, v.y) for v in face.bounding_poly.vertices]\n",
    "        print('Face surprised:', likelihood.name)\n",
    "        print('Face bounds:', \",\".join(vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/googlecodelabs/cloud-vision-python\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Additional Resources\r\n",
    "In addition to the source code for the four examples you completed in this codelab, below are additional reading material as well as recommended exercises to augment your knowledge and use of the Vision API with Python.\r\n",
    "\r\n",
    "Learn More\r\n",
    "Cloud Vision API documentation: cloud.google.com/vision/docs\r\n",
    "Cloud Vision API home page & live demo: cloud.google.com/vision\r\n",
    "Vision API label detection/annotation: cloud.google.com/vision/docs/labels\r\n",
    "Vision API facial feature recognition: cloud.google.com/vision/docs/detecting-faces\r\n",
    "Vision API landmark detection: cloud.google.com/vision/docs/detecting-landmarks\r\n",
    "Vision API optical character recognition (OCR): cloud.google.com/vision/docs/ocr\r\n",
    "Vision API \"Safe Search\": cloud.google.com/vision/docs/detecting-safe-search\r\n",
    "Vision API product/corporate logo detection: cloud.google.com/vision/docs/detecting-logos\r\n",
    "Python on Google Cloud Platform: cloud.google.com/python\r\n",
    "Google Cloud Python client: googlecloudplatform.github.io/google-cloud-python\r\n",
    "Codelab open source repo: github.com/googlecodelabs/cloud-vision-\r\n",
    "\r\n",
    "Additional Study\r\n",
    "Now that you have some experience with the Vision API under your belt, below are some recommended exercises to further develop your skills:\r\n",
    "\r\n",
    "You've built separate scripts demoing individual features of the Vision API. Combine at least 2 of them into another script. For example, add OCR/text recognition to the first script that performs label detection (label_detect.py). You may be surprised to find there is text on one of the hats in that image!\r\n",
    "\r\n",
    "Instead of our random images available on Google Cloud Storage, write a script that uses one or more of your images on your local filesystem. Another similar exercise is to find images online (accessible via http://).\r\n",
    "Same as #2, but with local images on your filesystem. Note that #2 may be an easier first step before doing this one with local files.\r\n",
    "\r\n",
    "Try non-photographs to see how the API works with those.\r\n",
    "\r\n",
    "Migrate some of the script functionality into a microservice hosted on Google Cloud Functions, or in a web app or mobile backend running on Google App Engine.\r\n",
    "\r\n",
    "If you're ready to tackle that last suggestion but can't think of any ideas, here are a pair to get your gears going:\r\n",
    "\r\n",
    "Analyze multiple images in a Cloud Storage bucket, a Google Drive folder (use the Drive API), or a directory on your local computer. Call the Vision API on each image, writing out data about each into a Google Sheet (use the Sheets API) or Excel spreadsheet. (NOTE: you may have to do some extra auth work as G Suite assets like Drive folders and Sheets spreadsheets generally belong to users, not service accounts.)\r\n",
    "\r\n",
    "Some people Tweet images (phone screenshots) of other tweets where the text of the original can't be cut-n-pasted or otherwise analyzed. Use the Twitter API to retrieve the referring tweet, extract and pass the tweeted image to the Vision API to OCR the text out of those images, then call the Cloud Natural Language API to perform sentiment analysis (to determine whether it's positive or negative) and entity extraction (search for entities/proper nouns) on them. (This is optional for the text in the referring tweet.)\r\n",
    "License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Unrestricted"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python391jvsc74a57bd08f0e6e8876776058453f48ad7c7dcdebdb8994a74dd96d3c38b5ba45b7b9008d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}