{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is:  ['fml-gcp-project-01']\n"
     ]
    }
   ],
   "source": [
    "PROJECT=!gcloud config get-value core/project \r\n",
    "#gcloud config list project --format \"value(core.project)\"\r\n",
    "print(\"Your current GCP Project Name is: \", PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating gs://pedok-ml-gcp-vision-api/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'pedok-ml-gcp-vision-api' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb gs://pedok-ml-gcp-vision-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://pedok-ml-gcp-vision-api/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable vision.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created service account [my-vision-sa].\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts create my-vision-sa --display-name \"my vision service account\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "created key [f03d7697ee42645571219830231c4f74fe285a5e] of type [json] as [key.json] for [my-vision-sa@fml-gcp-project-01.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts keys create key.json --iam-account my-vision-sa@fml-gcp-project-01.iam.gserviceaccount.com\r\n",
    "#!gcloud iam service-accounts keys create ~/key.json --iam-account my-vision-sa@{PROJECT_ID}.iam.gserviceaccount.com\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set GOOGLE_APPLICATION_CREDENTIALS=\"D:\\Portfolio\\my-projects\\GCP\\key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U pip google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2021-05-23T20:04:09.910Z'\n",
      "lifecycleState: ACTIVE\n",
      "name: fml-gcp-project-01\n",
      "projectId: fml-gcp-project-01\n",
      "projectNumber: '318144289060'\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects describe fml-gcp-project-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels (and confidence score):\n",
      "==============================\n",
      "Wheel (97.90%)\n",
      "Tire (97.85%)\n",
      "Bicycle (94.53%)\n",
      "Photograph (94.24%)\n",
      "Motor vehicle (91.37%)\n",
      "Infrastructure (89.83%)\n",
      "Bicycle wheel (85.65%)\n",
      "Mode of transport (85.51%)\n",
      "Asphalt (82.82%)\n",
      "Umbrella (81.78%)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\r\n",
    "\r\n",
    "image_uri = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\r\n",
    "\r\n",
    "client = vision.ImageAnnotatorClient()\r\n",
    "image = vision.Image()\r\n",
    "image.source.image_uri = image_uri\r\n",
    "\r\n",
    "response = client.label_detection(image=image)\r\n",
    "\r\n",
    "print('Labels (and confidence score):')\r\n",
    "print('=' * 30)\r\n",
    "for label in response.label_annotations:\r\n",
    "    print(label.description, '(%.2f%%)' % (label.score*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Perform Text Detection\r\n",
    "Text detection performs Optical Character Recognition (OCR). It detects and extracts text within an image with support for a broad range of languages. It also features automatic language identification.\r\n",
    "\r\n",
    "In this example, you will perform text detection on an image of an Otter Crossing. Copy the following snippet into your IPython session (or save locally as text_dectect.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "CAUTION\n",
      "Otters crossing\n",
      "for next 6 miles\n",
      "\n",
      "bounds: (61,243),(251,243),(251,340),(61,340)\n",
      "==============================\n",
      "CAUTION\n",
      "bounds: (75,245),(235,243),(235,269),(75,271)\n",
      "==============================\n",
      "Otters\n",
      "bounds: (65,296),(140,297),(140,315),(65,314)\n",
      "==============================\n",
      "crossing\n",
      "bounds: (151,295),(247,297),(247,318),(151,316)\n",
      "==============================\n",
      "for\n",
      "bounds: (61,322),(94,322),(94,340),(61,340)\n",
      "==============================\n",
      "next\n",
      "bounds: (106,322),(156,322),(156,340),(106,340)\n",
      "==============================\n",
      "6\n",
      "bounds: (167,321),(180,321),(180,339),(167,339)\n",
      "==============================\n",
      "miles\n",
      "bounds: (191,321),(251,321),(251,339),(191,339)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "image_uri = 'gs://cloud-vision-codelab/otter_crossing.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.text_detection(image=image)\n",
    "\n",
    "for text in response.text_annotations:\n",
    "    print('=' * 30)\n",
    "    print(text.description)\n",
    "    vertices = ['(%s,%s)' % (v.x, v.y) for v in text.bounding_poly.vertices]\n",
    "    print('bounds:', \",\".join(vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Perform Landmark Detection\n",
    "Landmark detection detects popular natural and man-made structures within an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "mid: \"/g/120xtw6z\"\n",
      "description: \"Trocad\\303\\251ro Gardens\"\n",
      "score: 0.9215528\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 339\n",
      "    y: 54\n",
      "  }\n",
      "  vertices {\n",
      "    x: 520\n",
      "    y: 54\n",
      "  }\n",
      "  vertices {\n",
      "    x: 520\n",
      "    y: 353\n",
      "  }\n",
      "  vertices {\n",
      "    x: 339\n",
      "    y: 353\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.861596299999995\n",
      "    longitude: 2.2892823\n",
      "  }\n",
      "}\n",
      "\n",
      "==============================\n",
      "mid: \"/m/02j81\"\n",
      "description: \"Eiffel Tower\"\n",
      "score: 0.65712523\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 458\n",
      "    y: 144\n",
      "  }\n",
      "  vertices {\n",
      "    x: 495\n",
      "    y: 144\n",
      "  }\n",
      "  vertices {\n",
      "    x: 495\n",
      "    y: 255\n",
      "  }\n",
      "  vertices {\n",
      "    x: 458\n",
      "    y: 255\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.858461\n",
      "    longitude: 2.294351\n",
      "  }\n",
      "}\n",
      "\n",
      "==============================\n",
      "mid: \"/m/02j81\"\n",
      "description: \"Eiffel Tower\"\n",
      "score: 0.65595245\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 448\n",
      "    y: 72\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 72\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 281\n",
      "  }\n",
      "  vertices {\n",
      "    x: 448\n",
      "    y: 281\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.875072\n",
      "    longitude: 2.312622\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "image_uri = 'gs://cloud-vision-codelab/eiffel_tower.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.landmark_detection(image=image)\n",
    "\n",
    "for landmark in response.landmark_annotations:\n",
    "    print('=' * 30)\n",
    "    print(landmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Perform Emotional Face Detection\r\n",
    "Facial features detection detects multiple faces within an image along with the associated key facial attributes such as emotional state or wearing headwear.\r\n",
    "\r\n",
    "In this example, you will detect the likelihood of emotional state from four different emotional likelihoods including: joy, anger, sorrow, and surprise.\r\n",
    "\r\n",
    "To perform emotional face detection, copy the following Python code into your IPython session (or save locally as face_dectect.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "File: face_surprise.jpg\n",
      "Face surprised: LIKELY\n",
      "Face bounds: (93,425),(520,425),(520,922),(93,922)\n",
      "==============================\n",
      "File: face_no_surprise.png\n",
      "Face surprised: VERY_UNLIKELY\n",
      "Face bounds: (120,0),(334,0),(334,198),(120,198)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from google.cloud import vision\n",
    "\n",
    "uri_base = 'gs://cloud-vision-codelab'\n",
    "pics = ('face_surprise.jpg', 'face_no_surprise.png')\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.Image()\n",
    "\n",
    "for pic in pics:\n",
    "    image.source.image_uri = '%s/%s' % (uri_base, pic)\n",
    "    response = client.face_detection(image=image)\n",
    "\n",
    "    print('=' * 30)\n",
    "    print('File:', pic)\n",
    "    for face in response.face_annotations:\n",
    "        likelihood = vision.Likelihood(face.surprise_likelihood)\n",
    "        vertices = ['(%s,%s)' % (v.x, v.y) for v in face.bounding_poly.vertices]\n",
    "        print('Face surprised:', likelihood.name)\n",
    "        print('Face bounds:', \",\".join(vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/googlecodelabs/cloud-vision-python\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Additional Resources\r\n",
    "In addition to the source code for the four examples you completed in this codelab, below are additional reading material as well as recommended exercises to augment your knowledge and use of the Vision API with Python.\r\n",
    "\r\n",
    "Learn More\r\n",
    "Cloud Vision API documentation: cloud.google.com/vision/docs\r\n",
    "Cloud Vision API home page & live demo: cloud.google.com/vision\r\n",
    "Vision API label detection/annotation: cloud.google.com/vision/docs/labels\r\n",
    "Vision API facial feature recognition: cloud.google.com/vision/docs/detecting-faces\r\n",
    "Vision API landmark detection: cloud.google.com/vision/docs/detecting-landmarks\r\n",
    "Vision API optical character recognition (OCR): cloud.google.com/vision/docs/ocr\r\n",
    "Vision API \"Safe Search\": cloud.google.com/vision/docs/detecting-safe-search\r\n",
    "Vision API product/corporate logo detection: cloud.google.com/vision/docs/detecting-logos\r\n",
    "Python on Google Cloud Platform: cloud.google.com/python\r\n",
    "Google Cloud Python client: googlecloudplatform.github.io/google-cloud-python\r\n",
    "Codelab open source repo: github.com/googlecodelabs/cloud-vision-\r\n",
    "\r\n",
    "Additional Study\r\n",
    "Now that you have some experience with the Vision API under your belt, below are some recommended exercises to further develop your skills:\r\n",
    "\r\n",
    "You've built separate scripts demoing individual features of the Vision API. Combine at least 2 of them into another script. For example, add OCR/text recognition to the first script that performs label detection (label_detect.py). You may be surprised to find there is text on one of the hats in that image!\r\n",
    "\r\n",
    "Instead of our random images available on Google Cloud Storage, write a script that uses one or more of your images on your local filesystem. Another similar exercise is to find images online (accessible via http://).\r\n",
    "Same as #2, but with local images on your filesystem. Note that #2 may be an easier first step before doing this one with local files.\r\n",
    "\r\n",
    "Try non-photographs to see how the API works with those.\r\n",
    "\r\n",
    "Migrate some of the script functionality into a microservice hosted on Google Cloud Functions, or in a web app or mobile backend running on Google App Engine.\r\n",
    "\r\n",
    "If you're ready to tackle that last suggestion but can't think of any ideas, here are a pair to get your gears going:\r\n",
    "\r\n",
    "Analyze multiple images in a Cloud Storage bucket, a Google Drive folder (use the Drive API), or a directory on your local computer. Call the Vision API on each image, writing out data about each into a Google Sheet (use the Sheets API) or Excel spreadsheet. (NOTE: you may have to do some extra auth work as G Suite assets like Drive folders and Sheets spreadsheets generally belong to users, not service accounts.)\r\n",
    "\r\n",
    "Some people Tweet images (phone screenshots) of other tweets where the text of the original can't be cut-n-pasted or otherwise analyzed. Use the Twitter API to retrieve the referring tweet, extract and pass the tweeted image to the Vision API to OCR the text out of those images, then call the Cloud Natural Language API to perform sentiment analysis (to determine whether it's positive or negative) and entity extraction (search for entities/proper nouns) on them. (This is optional for the text in the referring tweet.)\r\n",
    "License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision_v1\r\n",
    "\r\n",
    "image_uri = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\r\n",
    "\r\n",
    "client = vision_V1.ImageAnnotatorClient()\r\n",
    "\r\n",
    "source = {\"image_uri\": image_uri}\r\n",
    "image = {\"source\": source}\r\n",
    "\r\n",
    "features = [\r\n",
    "        {\"type_\": vision_v1.Feature.Type.LABEL_DETECTION},\r\n",
    "        {\"type_\": vision_v1.Feature.Type.IMAGE_PROPERTIES},\r\n",
    "    ]\r\n",
    "\r\n",
    "image = vision.Image()\r\n",
    "image.source.image_uri = image_uri\r\n",
    "\r\n",
    "response = client.label_detection(image=image)\r\n",
    "\r\n",
    "\r\n",
    "print('Labels (and confidence score):')\r\n",
    "print('=' * 30)\r\n",
    "for label in response.label_annotations:\r\n",
    "    print(label.description, '(%.2f%%)' % (label.score*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Unrestricted\r\n",
    "\r\n",
    "!pip install --user apache-beam[gcp]==2.16.0 \r\n",
    "!pip install --user tensorflow-transform==0.15.\r\n",
    "\r\n",
    "BUCKET = 'qwiklabs-gcp-01-89767ecc5125'\r\n",
    "PROJECT = 'qwiklabs-gcp-01-89767ecc5125'\r\n",
    "REGION = 'us-central1'\r\n",
    "\r\n",
    "import os\r\n",
    "os.environ['BUCKET'] = BUCKET\r\n",
    "os.environ['PROJECT'] = PROJECT\r\n",
    "os.environ['REGION'] = REGION\r\n",
    "\r\n",
    "gcloud config set project $PROJECT\r\n",
    "gcloud config set compute/region $REGION\r\n",
    "\r\n",
    "%%bash\r\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\r\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\r\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python391jvsc74a57bd08f0e6e8876776058453f48ad7c7dcdebdb8994a74dd96d3c38b5ba45b7b9008d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}