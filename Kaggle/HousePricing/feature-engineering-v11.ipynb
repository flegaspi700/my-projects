{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"papermill":{"duration":2.054366,"end_time":"2021-05-12T22:14:30.685243","exception":false,"start_time":"2021-05-12T22:14:28.630877","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","import os\n","import warnings\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from IPython.display import display\n","from pandas.api.types import CategoricalDtype\n","\n","from category_encoders import MEstimateEncoder\n","from category_encoders.cat_boost import CatBoostEncoder\n","from sklearn.cluster import KMeans\n","from sklearn import preprocessing\n","from sklearn.decomposition import PCA\n","from sklearn.feature_selection import mutual_info_regression\n","from sklearn.model_selection import RepeatedKFold\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import KFold, cross_val_score\n","from xgboost import XGBRegressor\n","\n","from scipy.stats import norm, skew\n","import sklearn.metrics as metrics\n","import math\n","\n","%matplotlib inline\n","\n","# Set Matplotlib defaults\n","plt.style.use(\"seaborn-whitegrid\")\n","plt.rc(\"figure\", autolayout=True)\n","plt.rc(\n","    \"axes\",\n","    labelweight=\"bold\",\n","    labelsize=\"large\",\n","    titleweight=\"bold\",\n","    titlesize=14,\n","    titlepad=10,\n",")\n","\n","# Mute warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031887,"end_time":"2021-05-12T22:14:30.749516","exception":false,"start_time":"2021-05-12T22:14:30.717629","status":"completed"},"tags":[]},"source":["## Data Preprocessing ##\n","\n","Before we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. The data we used in the course was a bit simpler than the competition data. For the *Ames* competition dataset, we'll need to:\n","- **Load** the data from CSV files\n","- **Clean** the data to fix any errors or inconsistencies\n","- **Encode** the statistical data type (numeric, categorical)\n","- **Impute** any missing values\n","\n","We'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.043079,"end_time":"2021-05-12T22:14:30.824996","exception":false,"start_time":"2021-05-12T22:14:30.781917","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def load_data():\n","    # Read data\n","    data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\n","    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n","    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=\"Id\")\n","    #df_train = pd.read_csv(\"train.csv\", index_col=\"Id\")\n","    #df_test = pd.read_csv(\"test.csv\", index_col=\"Id\")\n","    # Merge the splits so we can process them together\n","    df = pd.concat([df_train, df_test])\n","    # Preprocessing\n","    df = clean(df)\n","    df = encode(df)\n","    df = impute(df)\n","    \n","    drop_columns = ['MiscFeature', 'MiscVal', 'PoolQC', 'MoSold', 'Utilities', 'PoolArea', 'Condition2', 'Street']\n","    df.drop(drop_columns, axis=1, inplace=True)\n","    \n","    # Reform splits\n","    df_train = df.loc[df_train.index, :]\n","    df_test = df.loc[df_test.index, :]\n","    \n","    return df_train, df_test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031264,"end_time":"2021-05-12T22:14:30.888866","exception":false,"start_time":"2021-05-12T22:14:30.857602","status":"completed"},"tags":[]},"source":["### Clean Data ###\n","\n","Some of the categorical features in this dataset have what are apparently typos in their categories:"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031159,"end_time":"2021-05-12T22:14:31.081628","exception":false,"start_time":"2021-05-12T22:14:31.050469","status":"completed"},"tags":[]},"source":["Comparing these to `data_description.txt` shows us what needs cleaning. We'll take care of a couple of issues here, but you might want to evaluate this data further."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.041136,"end_time":"2021-05-12T22:14:31.154954","exception":false,"start_time":"2021-05-12T22:14:31.113818","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","def clean(df):\n","    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n","    # Some values of GarageYrBlt are corrupt, so we'll replace them\n","    # with the year the house was built\n","    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n","    # Names beginning with numbers are awkward to work with\n","    df.rename(columns={\n","        \"1stFlrSF\": \"FirstFlrSF\",\n","        \"2ndFlrSF\": \"SecondFlrSF\",\n","        \"3SsnPorch\": \"Threeseasonporch\",\n","    }, inplace=True,\n","    )\n","\n","    df.loc[df['MSZoning'] == \"C (all)\", 'MSZoning'] = \"C\"\n","\n","    return df"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.032265,"end_time":"2021-05-12T22:14:31.219085","exception":false,"start_time":"2021-05-12T22:14:31.18682","status":"completed"},"tags":[]},"source":["### Encode the Statistical Data Type ###\n","\n","Pandas has Python types corresponding to the standard statistical types (numeric, categorical, etc.). Encoding each feature with its correct type helps ensure each feature is treated appropriately by whatever functions we use, and makes it easier for us to apply transformations consistently. This hidden cell defines the `encode` function:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"papermill":{"duration":0.065339,"end_time":"2021-05-12T22:14:31.317904","exception":false,"start_time":"2021-05-12T22:14:31.252565","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","# The numeric features are already encoded correctly (`float` for\n","# continuous, `int` for discrete), but the categoricals we'll need to\n","# do ourselves. Note in particular, that the `MSSubClass` feature is\n","# read as an `int` type, but is actually a (nominative) categorical.\n","\n","# The nominative (unordered) categorical features \"Street\", \"Condition2\", \n","features_nom = [\"MSSubClass\", \"MSZoning\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"BldgType\", \"Heating\",\"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n","\n","\n","# The ordinal (ordered) categorical features \n","\n","# Pandas calls the categories \"levels\"\n","five_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n","ten_levels = list(range(10))\n","\n","ordered_levels = {\n","    \"OverallQual\": ten_levels,\n","    \"OverallCond\": ten_levels,\n","    \"ExterQual\": five_levels,\n","    \"ExterCond\": five_levels,\n","    \"BsmtQual\": five_levels,\n","    \"BsmtCond\": five_levels,\n","    \"HeatingQC\": five_levels,\n","    \"KitchenQual\": five_levels,\n","    \"FireplaceQu\": five_levels,\n","    \"GarageQual\": five_levels,\n","    \"GarageCond\": five_levels,\n","    \"PoolQC\": five_levels,\n","    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n","    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n","    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n","    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n","    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n","    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n","    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n","    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n","    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n","    \"CentralAir\": [\"N\", \"Y\"],\n","    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n","    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n","}\n","\n","# Add a None level for missing values\n","ordered_levels = {key: [\"None\"] + value for key, value in\n","                  ordered_levels.items()}\n","\n","\n","def encode(df):\n","    # Nominal categories\n","    for name in features_nom:\n","        df[name] = df[name].astype(\"category\")\n","        # Add a None category for missing values\n","        if \"None\" not in df[name].cat.categories:\n","            df[name].cat.add_categories(\"None\", inplace=True)\n","    # Ordinal categories\n","    for name, levels in ordered_levels.items():\n","        df[name] = df[name].astype(CategoricalDtype(levels,\n","                                                    ordered=True))\n","    return df"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.035951,"end_time":"2021-05-12T22:14:31.388656","exception":false,"start_time":"2021-05-12T22:14:31.352705","status":"completed"},"tags":[]},"source":["### Handle Missing Values ###\n","\n","Handling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `\"None\"` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.040457,"end_time":"2021-05-12T22:14:31.46069","exception":false,"start_time":"2021-05-12T22:14:31.420233","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def impute(df):\n","    #Assume Typical\n","    df['Functional'].fillna(value=\"Typ\", inplace=True)\n","\n","    df['LotFrontage'].fillna(df['LotFrontage'].mean(), inplace=True)\n","    df['GarageArea'].fillna(df['GarageArea'].mean(), inplace=True)\n","\n","    for column in df[['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd']]:\n","        df[column].fillna(df[column].mode()[0], inplace=True)\n","    \n","    for name in df.select_dtypes(\"number\"):\n","        df[name] = df[name].fillna(0)\n","\n","    for name in df.select_dtypes(\"category\"):\n","        df[name] = df[name].fillna(\"None\")\n","        \n","    return df"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031312,"end_time":"2021-05-12T22:14:31.523974","exception":false,"start_time":"2021-05-12T22:14:31.492662","status":"completed"},"tags":[]},"source":["## Load Data ##\n","\n","And now we can call the data loader and get the processed data splits:"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.320656,"end_time":"2021-05-12T22:14:31.876205","exception":false,"start_time":"2021-05-12T22:14:31.555549","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["df_train, df_test = load_data()\n","\n","# Reform splits\n","#df_train = df.loc[df_train.index, :]\n","#df_test = df.loc[df_test.index, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Peek at the values\n","#display(df_train)\n","#display(df_test)\n","\n","# Display information about dtypes and missing values\n","#display(df_train.info())\n","#display(df_test.info())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#print(df_train['SalePrice'].describe())\n","#sns.distplot(df_train['SalePrice'])\n","\n","#print(\"Skewness: %f\" % df_train['SalePrice'].skew())\n","#print(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#df_train['SalePrice'] = np.log1p(df_train['SalePrice'])\n","#sns.distplot(df_train['SalePrice'], fit=norm)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031569,"end_time":"2021-05-12T22:14:31.940035","exception":false,"start_time":"2021-05-12T22:14:31.908466","status":"completed"},"tags":[]},"source":["Uncomment and run this cell if you'd like to see what they contain. Notice that `df_test` is\n","missing values for `SalePrice`. (`NA`s were willed with 0's in the imputation step.)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Number of missing values in each column of training data\n","#missing_val_count_by_column = (df_train.isnull().sum())\n","#print(missing_val_count_by_column[missing_val_count_by_column > 0])\n","#missing_val_count_by_column = (df_test.isnull().sum())\n","#print(missing_val_count_by_column[missing_val_count_by_column > 0])"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.03666,"end_time":"2021-05-12T22:14:32.369571","exception":false,"start_time":"2021-05-12T22:14:32.332911","status":"completed"},"tags":[]},"source":["## Establish Baseline ##\n","\n","Finally, let's establish a baseline score to judge our feature engineering against.\n","\n","Here is the function we created in Lesson 1 that will compute the cross-validated RMSLE score for a feature set. We've used XGBoost for our model, but you might want to experiment with other models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"papermill":{"duration":0.046924,"end_time":"2021-05-12T22:14:32.452679","exception":false,"start_time":"2021-05-12T22:14:32.405755","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","def score_dataset(X, y, model=XGBRegressor()):\n","    # Label encoding for categoricals\n","    #\n","    # Label encoding is good for XGBoost and RandomForest, but one-hot\n","    # would be better for models like Lasso or Ridge. The `cat.codes`\n","    # attribute holds the category levels.\n","    for colname in X.select_dtypes([\"category\"]):\n","        X[colname] = X[colname].cat.codes\n","    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error) neg_mean_squared_error \n","    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n","    #cv = KFold(5,shuffle=True,random_state=42)\n","    log_y = np.log1p(y)\n","    score = cross_val_score(\n","        model, X, log_y, cv=cv, scoring=\"neg_mean_squared_error\",\n","    )\n","    score = -1 * score.mean()\n","    score = np.sqrt(score)\n","    return score\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.035944,"end_time":"2021-05-12T22:14:32.52509","exception":false,"start_time":"2021-05-12T22:14:32.489146","status":"completed"},"tags":[]},"source":["We can reuse this scoring function anytime we want to try out a new feature set. We'll run it now on the processed data with no additional features and get a baseline score:"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.776272,"end_time":"2021-05-12T22:14:34.337535","exception":false,"start_time":"2021-05-12T22:14:32.561263","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X = df_train.copy()\n","y = X.pop(\"SalePrice\")\n","\n","baseline_score = score_dataset(X, y)\n","print(f\"Baseline score: {baseline_score:.5f} RMSLE\")\n","\n","#0.14082 RMSLE\n","#Baseline score: 0.01196 RMSLE np.log SalesPrice only\n","#Baseline score: 0.00238 RMSLE np.log Most Skewed features"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.03823,"end_time":"2021-05-12T22:14:34.414712","exception":false,"start_time":"2021-05-12T22:14:34.376482","status":"completed"},"tags":[]},"source":["This baseline score helps us to know whether some set of features we've assembled has actually led to any improvement or not.\n","\n","# Step 2 - Feature Utility Scores #\n","\n","In Lesson 2 we saw how to use mutual information to compute a *utility score* for a feature, giving you an indication of how much potential the feature has. This hidden cell defines the two utility functions we used, `make_mi_scores` and `plot_mi_scores`: "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"papermill":{"duration":0.047709,"end_time":"2021-05-12T22:14:34.499106","exception":false,"start_time":"2021-05-12T22:14:34.451397","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","def make_mi_scores(X, y):\n","    X = X.copy()\n","    for colname in X.select_dtypes([\"object\", \"category\"]):\n","        X[colname], _ = X[colname].factorize()\n","    # All discrete features should now have integer dtypes\n","    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n","    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n","    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n","    mi_scores = mi_scores.sort_values(ascending=False)\n","    return mi_scores\n","\n","\n","def plot_mi_scores(scores):\n","    scores = scores.sort_values(ascending=True)\n","    width = np.arange(len(scores))\n","    ticks = list(scores.index)\n","    plt.barh(width, scores)\n","    plt.yticks(width, ticks)\n","    plt.title(\"Mutual Information Scores\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.036594,"end_time":"2021-05-12T22:14:34.572853","exception":false,"start_time":"2021-05-12T22:14:34.536259","status":"completed"},"tags":[]},"source":["Let's look at our feature scores again:"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.760589,"end_time":"2021-05-12T22:14:36.370405","exception":false,"start_time":"2021-05-12T22:14:34.609816","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X = df_train.copy()\n","y = X.pop(\"SalePrice\")\n","\n","mi_scores = make_mi_scores(X, y)\n","mi_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#print(\"Top 20 MI:\")\n","#print(mi_scores.head(20))\n","#print(\"\\n\")\n","#print(\"Bootom 20 MI:\")\n","#print(mi_scores.tail(20))  # uncomment to see bottom 20\n","\n","#plt.figure(dpi=100, figsize=(8, 5))\n","#plot_mi_scores(mi_scores.head(20))\n","#plt.figure(dpi=100, figsize=(8, 5))\n","#plot_mi_scores(mi_scores.tail(20))  # uncomment to see bottom 20"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037979,"end_time":"2021-05-12T22:14:36.445628","exception":false,"start_time":"2021-05-12T22:14:36.407649","status":"completed"},"tags":[]},"source":["You can see that we have a number of features that are highly informative and also some that don't seem to be informative at all (at least by themselves). As we talked about in Tutorial 2, the top scoring features will usually pay-off the most during feature development, so it could be a good idea to focus your efforts on those. On the other hand, training on uninformative features can lead to overfitting. So, the features with 0.0 scores we'll drop entirely:"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.048843,"end_time":"2021-05-12T22:14:36.532261","exception":false,"start_time":"2021-05-12T22:14:36.483418","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def drop_uninformative(df, mi_scores):\n","    return df.loc[:, mi_scores > 0.0]\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037486,"end_time":"2021-05-12T22:14:36.607382","exception":false,"start_time":"2021-05-12T22:14:36.569896","status":"completed"},"tags":[]},"source":["Removing them does lead to a modest performance gain:"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.712366,"end_time":"2021-05-12T22:14:38.357228","exception":false,"start_time":"2021-05-12T22:14:36.644862","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X = df_train.copy()\n","y = X.pop(\"SalePrice\")\n","X = drop_uninformative(X, mi_scores)\n","\n","score_dataset(X, y)\n","#0.14338026718687277\n","#0.012029091210479594 np log Salesprice only\n","#0.00235681486982465 with nplog most skewed features"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037685,"end_time":"2021-05-12T22:14:38.434974","exception":false,"start_time":"2021-05-12T22:14:38.397289","status":"completed"},"tags":[]},"source":["Later, we'll add the `drop_uninformative` function to our feature-creation pipeline.\n","\n","# Step 3 - Create Features #\n","\n","Now we'll start developing our feature set.\n","\n","To make our feature engineering workflow more modular, we'll define a function that will take a prepared dataframe and pass it through a pipeline of transformations to get the final feature set. It will look something like this:\n","\n","```\n","def create_features(df):\n","    X = df.copy()\n","    y = X.pop(\"SalePrice\")\n","    X = X.join(create_features_1(X))\n","    X = X.join(create_features_2(X))\n","    X = X.join(create_features_3(X))\n","    # ...\n","    return X\n","```\n","\n","Let's go ahead and define one transformation now, a [label encoding](https://www.kaggle.com/alexisbcook/categorical-variables) for the categorical features:"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.046709,"end_time":"2021-05-12T22:14:38.519702","exception":false,"start_time":"2021-05-12T22:14:38.472993","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def label_encode(df):\n","    X = df.copy()\n","    for colname in X.select_dtypes([\"category\"]):\n","        X[colname] = X[colname].cat.codes\n","    return X\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037724,"end_time":"2021-05-12T22:14:38.595413","exception":false,"start_time":"2021-05-12T22:14:38.557689","status":"completed"},"tags":[]},"source":["A label encoding is okay for any kind of categorical feature when you're using a tree-ensemble like XGBoost, even for unordered categories. If you wanted to try a linear regression model (also popular in this competition), you would instead want to use a one-hot encoding, especially for the features with unordered categories.\n","\n","## Create Features with Pandas ##\n","\n","This cell reproduces the work you did in Exercise 3, where you applied strategies for creating features in Pandas. Modify or add to these functions to try out other feature combinations."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#display(df_train.info())\n","#print(mi_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def mathematical_transforms(df):\n","    X = pd.DataFrame()  # dataframe to hold new features\n","    X[\"LivLotRatio\"] = df.GrLivArea / df.LotArea\n","    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) / df.TotRmsAbvGrd\n","    X['TotalSF'] = df['TotalBsmtSF'] + df['FirstFlrSF'] + df['SecondFlrSF']\n","    #X['OverallQualCond'] = df['OverallQual'].cat.codes * df['OverallCond'].cat.codes\n","    X['ExterQualCond'] = df['ExterQual'].cat.codes * df['ExterCond'].cat.codes  \n","    X['BsmtQualCond'] = df['BsmtQual'].cat.codes * df['BsmtCond'].cat.codes\n","    X['GarageQualCond'] = df['GarageQual'].cat.codes * df['GarageCond'].cat.codes\n","    X['OtherQualCond'] = df['HeatingQC'].cat.codes * df['KitchenQual'].cat.codes * df['FireplaceQu'].cat.codes\n","    return X\n","\n","def counts(df):\n","    X = pd.DataFrame()\n","    X[\"PorchTypes\"] = df[[\n","        \"WoodDeckSF\",\n","        \"OpenPorchSF\",\n","        \"EnclosedPorch\",\n","        \"Threeseasonporch\",\n","        \"ScreenPorch\",\n","    ]].gt(0.0).sum(axis=1)\n","    return X\n","\n","\n","def group_transforms(df):\n","    X = pd.DataFrame()\n","    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n","    return X\n","\n","def interactions(df):\n","    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n","    X = X.mul(df.GrLivArea, axis=0)\n","    return X\n","\n","\n","#df_train['LotAreaSqrt'] = math.sqrt(df_train['LotArea'])\n","#df_test['LotAreaSqrt'] = math.sqrt(df_test['LotArea'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Label Encode\n","for colname in X.select_dtypes([\"category\"]):\n","    X[colname] = X[colname].cat.codes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#df_train.shape , df_test.shape \n","#df_train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X = df_train.copy()\n","y = df_train.loc[:, \"SalePrice\"]\n","\n","score_dataset(X, y)\n","#0.011912967179218928\n","#0.011958654827824274\n","#0.011523066377609595\n","#0.004221350648558785\n","#0.004222141778371656\n","#0.004246875415316747\n","#0.00415054270288803"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037364,"end_time":"2021-05-12T22:14:38.760334","exception":false,"start_time":"2021-05-12T22:14:38.72297","status":"completed"},"tags":[]},"source":["Here are some ideas for other transforms you could explore:\n","- Interactions between the quality `Qual` and condition `Cond` features. `OverallQual`, for instance, was a high-scoring feature. You could try combining it with `OverallCond` by converting both to integer type and taking a product.\n","- Square roots of area features. This would convert units of square feet to just feet.\n","- Logarithms of numeric features. If a feature has a skewed distribution, applying a logarithm can help normalize it.\n","- Interactions between numeric and categorical features that describe the same thing. You could look at interactions between `BsmtQual` and `TotalBsmtSF`, for instance.\n","- Other group statistics in `Neighboorhood`. We did the median of `GrLivArea`. Looking at `mean`, `std`, or `count` could be interesting. You could also try combining the group statistics with other features. Maybe the *difference* of `GrLivArea` and the median is important?\n","\n","## k-Means Clustering ##\n","\n","The first unsupervised algorithm we used to create features was k-means clustering. We saw that you could either use the cluster labels as a feature (a column with `0, 1, 2, ...`) or you could use the *distance* of the observations to each cluster. We saw how these features can sometimes be effective at untangling complicated spatial relationships."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"lines_to_next_cell":2,"papermill":{"duration":0.050169,"end_time":"2021-05-12T22:14:38.848804","exception":false,"start_time":"2021-05-12T22:14:38.798635","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","cluster_features = [\n","    \"LotArea\",\n","    \"TotalBsmtSF\",\n","    \"FirstFlrSF\",\n","    \"SecondFlrSF\",\n","    \"GrLivArea\",\n","]\n","\n","\n","def cluster_labels(df, features, n_clusters=20):\n","    X = df.copy()\n","    X_scaled = X.loc[:, features]\n","    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n","    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n","    X_new = pd.DataFrame()\n","    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n","    return X_new\n","\n","\n","def cluster_distance(df, features, n_clusters=20):\n","    X = df.copy()\n","    X_scaled = X.loc[:, features]\n","    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n","    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n","    X_cd = kmeans.fit_transform(X_scaled)\n","    # Label features and join to dataset\n","    X_cd = pd.DataFrame(\n","        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n","    )\n","    return X_cd\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.038059,"end_time":"2021-05-12T22:14:38.925043","exception":false,"start_time":"2021-05-12T22:14:38.886984","status":"completed"},"tags":[]},"source":["## Principal Component Analysis ##\n","\n","PCA was the second unsupervised model we used for feature creation. We saw how it could be used to decompose the variational structure in the data. The PCA algorithm gave us *loadings* which described each component of variation, and also the *components* which were the transformed datapoints. The loadings can suggest features to create and the components we can use as features directly.\n","\n","Here are the utility functions from the PCA lesson:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"papermill":{"duration":0.052411,"end_time":"2021-05-12T22:14:39.015097","exception":false,"start_time":"2021-05-12T22:14:38.962686","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","def apply_pca(X, standardize=True):\n","    # Standardize\n","    if standardize:\n","        X = (X - X.mean(axis=0)) / X.std(axis=0)\n","    # Create principal components\n","    pca = PCA()\n","    X_pca = pca.fit_transform(X)\n","    # Convert to dataframe\n","    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n","    X_pca = pd.DataFrame(X_pca, columns=component_names)\n","    # Create loadings\n","    loadings = pd.DataFrame(\n","        pca.components_.T,  # transpose the matrix of loadings\n","        columns=component_names,  # so the columns are the principal components\n","        index=X.columns,  # and the rows are the original features\n","    )\n","    return pca, X_pca, loadings\n","\n","\n","def plot_variance(pca, width=8, dpi=100):\n","    # Create figure\n","    fig, axs = plt.subplots(1, 2)\n","    n = pca.n_components_\n","    grid = np.arange(1, n + 1)\n","    # Explained variance\n","    evr = pca.explained_variance_ratio_\n","    axs[0].bar(grid, evr)\n","    axs[0].set(\n","        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n","    )\n","    # Cumulative Variance\n","    cv = np.cumsum(evr)\n","    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n","    axs[1].set(\n","        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n","    )\n","    # Set up figure\n","    fig.set(figwidth=8, dpi=100)\n","    return axs\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037964,"end_time":"2021-05-12T22:14:39.091474","exception":false,"start_time":"2021-05-12T22:14:39.05351","status":"completed"},"tags":[]},"source":["And here are transforms that produce the features from the Exercise 5. You might want to change these if you came up with a different answer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"papermill":{"duration":0.049201,"end_time":"2021-05-12T22:14:39.178807","exception":false,"start_time":"2021-05-12T22:14:39.129606","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","def pca_inspired(df):\n","    X = pd.DataFrame()\n","    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n","    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n","    return X\n","\n","\n","def pca_components(df, features):\n","    X = df.loc[:, features]\n","    _, X_pca, _ = apply_pca(X)\n","    return X_pca\n","\n","\n","pca_features = [\n","    \"GarageArea\",\n","    \"YearRemodAdd\",\n","    \"TotalBsmtSF\",\n","    \"GrLivArea\",\n","]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.040878,"end_time":"2021-05-12T22:14:39.259131","exception":false,"start_time":"2021-05-12T22:14:39.218253","status":"completed"},"tags":[]},"source":["These are only a couple ways you could use the principal components. You could also try clustering using one or more components. One thing to note is that PCA doesn't change the distance between points -- it's just like a rotation. So clustering with the full set of components is the same as clustering with the original features. Instead, pick some subset of components, maybe those with the most variance or the highest MI scores.\n","\n","For further analysis, you might want to look at a correlation matrix for the dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.492852,"end_time":"2021-05-12T22:14:40.795497","exception":false,"start_time":"2021-05-12T22:14:39.302645","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n","    sns.clustermap(\n","        df.corr(method),\n","        vmin=-1.0,\n","        vmax=1.0,\n","        cmap=\"icefire\",\n","        method=\"complete\",\n","        annot=annot,\n","        **kwargs,\n","    )\n","\n","\n","#corrplot(df_train, annot=None)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.042834,"end_time":"2021-05-12T22:14:40.880918","exception":false,"start_time":"2021-05-12T22:14:40.838084","status":"completed"},"tags":[]},"source":["Groups of highly correlated features often yield interesting loadings.\n","\n","### PCA Application - Indicate Outliers ###\n","\n","In Exercise 5, you applied PCA to determine houses that were **outliers**, that is, houses having values not well represented in the rest of the data. You saw that there was a group of houses in the `Edwards` neighborhood having a `SaleCondition` of `Partial` whose values were especially extreme.\n","\n","Some models can benefit from having these outliers indicated, which is what this next transform will do."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.050807,"end_time":"2021-05-12T22:14:40.974018","exception":false,"start_time":"2021-05-12T22:14:40.923211","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def indicate_outliers(df):\n","    X_new = pd.DataFrame()\n","    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n","    return X_new\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.041469,"end_time":"2021-05-12T22:14:41.057793","exception":false,"start_time":"2021-05-12T22:14:41.016324","status":"completed"},"tags":[]},"source":["You could also consider applying some sort of robust scaler from scikit-learn's `sklearn.preprocessing` module to the outlying values, especially those in `GrLivArea`. [Here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) is a tutorial illustrating some of them. Another option could be to create a feature of \"outlier scores\" using one of scikit-learn's [outlier detectors](https://scikit-learn.org/stable/modules/outlier_detection.html)."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.041296,"end_time":"2021-05-12T22:14:41.140973","exception":false,"start_time":"2021-05-12T22:14:41.099677","status":"completed"},"tags":[]},"source":["## Target Encoding ##\n","\n","Needing a separate holdout set to create a target encoding is rather wasteful of data. In *Tutorial 6* we used 25% of our dataset just to encode a single feature, `Zipcode`. The data from the other features in that 25% we didn't get to use at all.\n","\n","There is, however, a way you can use target encoding without having to use held-out encoding data. It's basically the same trick used in cross-validation:\n","1. Split the data into folds, each fold having two splits of the dataset.\n","2. Train the encoder on one split but transform the values of the other.\n","3. Repeat for all the splits.\n","\n","This way, training and transformation always take place on independent sets of data, just like when you use a holdout set but without any data going to waste.\n","\n","In the next hidden cell is a wrapper you can use with any target encoder:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"papermill":{"duration":0.057657,"end_time":"2021-05-12T22:14:41.240452","exception":false,"start_time":"2021-05-12T22:14:41.182795","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","class CrossFoldEncoder:\n","    def __init__(self, encoder, **kwargs):\n","        self.encoder_ = encoder\n","        self.kwargs_ = kwargs  # keyword arguments for the encoder\n","        self.cv_ = KFold(n_splits=5)\n","\n","    # Fit an encoder on one split and transform the feature on the\n","    # other. Iterating over the splits in all folds gives a complete\n","    # transformation. We also now have one trained encoder on each\n","    # fold.\n","    def fit_transform(self, X, y, cols):\n","        self.fitted_encoders_ = []\n","        self.cols_ = cols\n","        X_encoded = []\n","        for idx_encode, idx_train in self.cv_.split(X):\n","            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n","            fitted_encoder.fit(\n","                X.iloc[idx_encode, :], y.iloc[idx_encode],\n","            )\n","            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n","            self.fitted_encoders_.append(fitted_encoder)\n","        X_encoded = pd.concat(X_encoded)\n","        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n","        return X_encoded\n","\n","    # To transform the test data, average the encodings learned from\n","    # each fold.\n","    def transform(self, X):\n","        from functools import reduce\n","\n","        X_encoded_list = []\n","        for fitted_encoder in self.fitted_encoders_:\n","            X_encoded = fitted_encoder.transform(X)\n","            X_encoded_list.append(X_encoded[self.cols_])\n","        X_encoded = reduce(\n","            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n","        ) / len(X_encoded_list)\n","        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n","        return X_encoded\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.041575,"end_time":"2021-05-12T22:14:41.325608","exception":false,"start_time":"2021-05-12T22:14:41.284033","status":"completed"},"tags":[]},"source":["Use it like:\n","\n","```\n","encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n","X_encoded = encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n","```\n","\n","You can turn any of the encoders from the [`category_encoders`](http://contrib.scikit-learn.org/category_encoders/) library into a cross-fold encoder. The [`CatBoostEncoder`](http://contrib.scikit-learn.org/category_encoders/catboost.html) would be worth trying. It's similar to `MEstimateEncoder` but uses some tricks to better prevent overfitting. Its smoothing parameter is called `a` instead of `m`.\n","\n","## Create Final Feature Set ##\n","\n","Now let's combine everything together. Putting the transformations into separate functions makes it easier to experiment with various combinations. The ones I left uncommented I found gave the best results. You should experiment with you own ideas though! Modify any of these transformations or come up with some of your own to add to the pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":4.811913,"end_time":"2021-05-12T22:14:46.179325","exception":false,"start_time":"2021-05-12T22:14:41.367412","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def create_features(df, df_test=None):\n","    X = df.copy()\n","    y = X.pop(\"SalePrice\")\n","    mi_scores = make_mi_scores(X, y)\n","\n","    # Combine splits if test data is given\n","    #\n","    # If we're creating features for test set predictions, we should\n","    # use all the data we have available. After creating our features,\n","    # we'll recreate the splits.\n","    if df_test is not None:\n","        X_test = df_test.copy()\n","        X_test.pop(\"SalePrice\")\n","        X = pd.concat([X, X_test])\n","\n","    # Lesson 2 - Mutual Information\n","    X = drop_uninformative(X, mi_scores)\n","\n","    # Lesson 3 - Transformations\n","    X = X.join(mathematical_transforms(X))\n","    X = X.join(interactions(X))\n","    X = X.join(counts(X))\n","    # X = X.join(break_down(X))\n","    X = X.join(group_transforms(X))\n","        \n","    # Lesson 4 - Clustering\n","    #X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n","    #X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n","\n","    # Lesson 5 - PCA\n","    X = X.join(pca_inspired(X))\n","    #X = X.join(pca_components(X, pca_features))\n","    X = X.join(indicate_outliers(X))\n","\n","    X = label_encode(X)\n","    \n","    numeric_feats = df.dtypes[df.dtypes != 'category'].index\n","    skewed_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n","    high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n","    #print(high_skew)\n","\n","    for feature in high_skew.index:\n","        df[feature] = np.log1p(df[feature])\n","        \n","    # Reform splits\n","    if df_test is not None:\n","        X_test = X.loc[df_test.index, :]\n","        X.drop(df_test.index, inplace=True)\n","\n","    # Lesson 6 - Target Encoder\n","    encoder = CrossFoldEncoder(CatBoostEncoder, a=1)\n","    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n","    if df_test is not None:\n","        X_test = X_test.join(encoder.transform(X_test))\n","\n","    if df_test is not None:\n","        return X, X_test\n","    else:\n","        return X\n","\n","\n","df_train, df_test = load_data()\n","X_train = create_features(df_train)\n","y_train = df_train.loc[:, \"SalePrice\"]\n","\n","y_train = np.log1p(y_train)\n","\n","score_dataset(X_train, y_train)\n","#0.1381925629969659 - base code\n","#0.01174280926525156 - with log - Saleprice with interactions without drop\n","#0.011788791015551485 - without interactions without drop\n","#0.011411558694407888\n","#0.011334418067498464"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#numeric_feats = X_train.dtypes[X_train.dtypes != 'category'].index\n","#skewed_feats = X_train[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n","#high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n","#high_skew\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#sns.distplot(X_train['Condition2'], fit=norm)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#sns.distplot(y_train, fit=norm)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.043414,"end_time":"2021-05-12T22:14:46.266417","exception":false,"start_time":"2021-05-12T22:14:46.223003","status":"completed"},"tags":[]},"source":["# Step 4 - Hyperparameter Tuning #\n","\n","At this stage, you might like to do some hyperparameter tuning with XGBoost before creating your final submission."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","import optuna\n","\n","def objective(trial):\n","       \n","    X_train = create_features(df_train)\n","    y_train = df_train.loc[:, \"SalePrice\"]\n","    \n","    y_train = np.log1p(y_train)\n"," \n","    xgb_params_2 = dict(\n","        random_state=trial.suggest_int(\"random_state\", 0, 3), \n","        max_depth=trial.suggest_int(\"max_depth\", 1, 10),\n","        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n","        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 3200),\n","        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n","        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n","        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n","        gamma=trial.suggest_float(\"gamma\", 0.2, 1.0), \n","        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n","        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n","        nthread = -1,     \n","    )\n","    xgb_2 = XGBRegressor(**xgb_params_2) #XGBRegressor\n","    return score_dataset(X_train, y_train, xgb_2)\n","\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=50)\n","\n","trial = study.best_trial\n","print('Accuracy: {}'.format(trial.value))\n","print(\"Best hyperparameters: {}\".format(trial.params))\n","\n","\n","#Trial 16 finished with value: 0.009990301624146384 and parameters: {'random_state': 0, \n","#'num_parallel_tree': 2, 'max_depth': 6, 'learning_rate': 0.03425294775337941, \n","#'n_estimators': 2156, 'min_child_weight': 2, 'colsample_bytree': 0.5359155800868783, \n","#'subsample': 0.6779118064142821, 'reg_alpha': 0.00011170341882253135, 'reg_lambda': 0.001371195698120206}. \n","#Best is trial 16 with value: 0.009990301624146384.\n","    \n","#Trial 11 finished with value: 0.008599099860733489 and parameters: {'random_state': 2, \n","#'max_depth': 1, 'learning_rate': 0.08976027722263541, 'n_estimators': 1863, \n","#            'min_child_weight': 6, 'colsample_bytree': 0.2479280294188766, '\n","#                    subsample': 0.8168463784140053, 'gamma': 0.3517995495865719, \n","#                    'reg_alpha': 0.09773043448294143, 'reg_lambda': 0.04289740192308445}. \n","#                            Best is trial 11 with value: 0.008599099860733489.\n","#colsample_bytree=0.4603, gamma=0.0468, \n","#                  learning_rate=0.05, max_depth=3, \n","#                  min_child_weight=1.7817, n_estimators=2200,\n","#                  reg_alpha=0.4640, reg_lambda=0.8571,subsample=0.5213,\n","#                  random_state =7, nthread = -1\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":90.528888,"end_time":"2021-05-12T22:16:16.837056","exception":false,"start_time":"2021-05-12T22:14:46.308168","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_train = create_features(df_train)\n","y_train = df_train.loc[:, \"SalePrice\"]\n","\n","#numeric_feats = X_train.dtypes[X_train.dtypes != 'category'].index\n","#skewed_feats = X_train[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n","#high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n","#high_skew\n","#\n","#for feature in high_skew.index:\n","#    X_train[feature] = np.log1p(X_train[feature])\n","\n","y_train = np.log1p(y_train)\n","\n","xgb_params = dict(random_state=2, \n","                  max_depth=1, \n","                  learning_rate=0.08976027722263541, \n","                  n_estimators=1863, \n","                  min_child_weight= 6, \n","                  colsample_bytree=0.2479280294188766, \n","                  subsample=0.8168463784140053, \n","                  gamma=0.3517995495865719, \n","                  reg_alpha=0.09773043448294143, \n","                  reg_lambda=0.04289740192308445, \n","                  nthread = -1)\n","#**xgb_params\n","xgb = XGBRegressor(**xgb_params)\n","score_dataset(X_train, y_train, xgb)\n","#0.12414985267470383 base code\n","#0.11671757008905952\n","#0.009990301624146384"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# max_depth=6,           # maximum depth of each tree - try 2 to 10\n","#    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n","#    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n","#    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n","#    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n","#    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n","#    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n","#   reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n","#   num_parallel_tree=1,   # set > 1 for boosted random forests"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.043221,"end_time":"2021-05-12T22:16:16.924095","exception":false,"start_time":"2021-05-12T22:16:16.880874","status":"completed"},"tags":[]},"source":["Just tuning these by hand can give you great results. However, you might like to try using one of scikit-learn's automatic [hyperparameter tuners](https://scikit-learn.org/stable/modules/grid_search.html). Or you could explore more advanced tuning libraries like [Optuna](https://optuna.readthedocs.io/en/stable/index.html) or [scikit-optimize](https://scikit-optimize.github.io/stable/).\n","\n"," [Optuna's visualizations](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":23.582128,"end_time":"2021-05-12T22:16:40.549813","exception":false,"start_time":"2021-05-12T22:16:16.967685","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","X_train, X_test = create_features(df_train, df_test)\n","y_train = df_train.loc[:, \"SalePrice\"]\n","\n","#numeric_feats = X_train.dtypes[X_train.dtypes != 'category'].index\n","#skewed_feats = X_train[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n","#high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n","\n","\n","#for feature in high_skew.index:\n","#    X_train[feature] = np.log1p(X_train[feature])\n","\n","y_train = np.log1p(y_train)\n","\n","xgb = XGBRegressor(**xgb_params)\n","# XGB minimizes MSE, but competition loss is RMSLE\n","# So, we need to log-transform y to train and exp-transform the predictions\n","xgb.fit(X_train, np.log1p(y))\n","#predictions = np.exp(xgb.predict(X_test))\n","predictions = np.floor(np.expm1(xgb.predict(X_test)))\n","\n","output = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\n","output.to_csv('my_submission.csv', index=False)\n","print(\"Your submission was successfully saved!\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.1 64-bit","name":"python391jvsc74a57bd08f0e6e8876776058453f48ad7c7dcdebdb8994a74dd96d3c38b5ba45b7b9008d"},"language_info":{"name":"python","version":""}},"nbformat":4,"nbformat_minor":4}